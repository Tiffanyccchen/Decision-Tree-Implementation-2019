{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to accompany Machine Learning Recipes #8. We'll write a Decision Tree Classifier, in pure Python. Below each of the methods, I've written a little demo to help explain what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Python 2/3 compatability\n",
    "from __future__ import print_function\n",
    "from math import log\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n18\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['mid', 'mid', 'good', 'mid', 'unstable', 'stable', 'unstable', 10, 'S']"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# data reading\n",
    "my_file = open('postoperative.txt','r')\n",
    "lines = my_file.readlines()\n",
    "\n",
    "data = [[] for i in range(len(lines))]\n",
    "\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    lines[i]=lines[i][:-1]\n",
    "    lines[i]=lines[i].split(',')\n",
    "\n",
    "    if lines[i][7]=='?':\n",
    "        lines[i][7]=10\n",
    "    if lines[i][8]==\"I\":\n",
    "        continue\n",
    "    if lines[i][8]==\"A \":\n",
    "        lines[i][8]='A'\n",
    "\n",
    "    lines[i][7]=int(lines[i][7])\n",
    "    data[i].append(lines[i])\n",
    "    data[i]=sum(data[i], [])\n",
    "    \n",
    "data[0][0]='mid'\n",
    "data[-1][-1]='S'\n",
    "data=[x for x in data if x != []]\n",
    "\n",
    "## split training and testing\n",
    "randtran=random.sample(range(88),70)\n",
    "testing_data=[data[i] for i in range(88) if i not in randtran]\n",
    "training_data=[data[num] for num in randtran]\n",
    "\n",
    "print(len(training_data))\n",
    "print(len(testing_data))\n",
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy dataset.\n",
    "# Format: each row is an example.\n",
    "# The last column is the label.\n",
    "# The first two columns are features.\n",
    "# Feel free to play with it by adding more features & examples.\n",
    "# Interesting note: I've written this so the 2nd and 5th examples\n",
    "# have the same features, but different labels - so we can see how the\n",
    "# tree handles this case.\n",
    "#training_data = [\n",
    "#    ['Green', 3, 'Apple'],\n",
    "#    ['Yellow', 3, 'Apple'],\n",
    "#    ['Red', 1, 'Grape'],\n",
    "#    ['Red', 1, 'Grape'],\n",
    "#    ['Yellow', 3, 'Grape'],\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column labels.\n",
    "# These are used only to print the tree.\n",
    "header = [\"L-CORE\", \"L-SURF\",\"L-O2\",\"L-BP\",\"SURF-STBL\",\"CORE-STBL\", \"BP-STBL\",'COMFORT','Decision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_vals(rows, col):\n",
    "    \"\"\"Find the unique values for a column in a dataset.\"\"\"\n",
    "    return set([row[col] for row in rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'high', 'low', 'mid'}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "#######\n",
    "# Demo:\n",
    "unique_vals(training_data,0)\n",
    "# unique_vals(training_data, 1)\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['S', 'A']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "uni_label=list(unique_vals(training_data,-1))\n",
    "uni_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_counts(rows):\n",
    "    \"\"\"Counts the number of each type of example in a dataset.\"\"\"\n",
    "    counts = {}  # a dictionary of label -> count.\n",
    "    for row in rows:\n",
    "        # in our dataset format, the label is always the last column\n",
    "        label = row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('S', 17), ('A', 53)]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "#######\n",
    "# Demo:\n",
    "class_counts(training_data)\n",
    "list(class_counts(training_data).items())\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(value):\n",
    "    \"\"\"Test if a value is numeric.\"\"\"\n",
    "    return isinstance(value, int) or isinstance(value, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "#######\n",
    "# Demo:\n",
    "is_numeric(training_data[0][7])\n",
    "# is_numeric(\"Red\")\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question:\n",
    "    \"\"\"A Question is used to partition a dataset.\n",
    "\n",
    "    This class just records a 'column number' (e.g., 0 for Color) and a\n",
    "    'column value' (e.g., Green). The 'match' method is used to compare\n",
    "    the feature value in an example to the feature value stored in the\n",
    "    question. See the demo below.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column, value):\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "\n",
    "    def match(self, example):\n",
    "        # Compare the feature value in an example to the\n",
    "        # feature value in this question.\n",
    "        val = example[self.column]\n",
    "        return val >= self.value\n",
    "\n",
    "    def __repr__(self):\n",
    "        # This is just a helper method to print\n",
    "        # the question in a readable format.\n",
    "            condition = \">=\"\n",
    "            return \"Is %s %s %s?\" % (\n",
    "                header[self.column], condition, str(self.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Questionnom:\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "    def __repr__(self):\n",
    "        return \"What's the %s of instances?\" % (header[self.column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Is COMFORT >= 10?"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "#######\n",
    "# Demo:\n",
    "# Let's write a question for a numeric attribute\n",
    "Question(7, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "What's the L-CORE of instances?"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "Questionnom(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(rows, question):\n",
    "    \"\"\"Partitions a dataset for numerical attributes.\n",
    "\n",
    "    For each row in the dataset, check if it matches the question. If\n",
    "    so, add it to 'true rows', otherwise, add it to 'false rows'.\n",
    "    \"\"\"\n",
    "    true_rows, false_rows = [], []\n",
    "    for row in rows:\n",
    "        if question.match(row):\n",
    "            true_rows.append(row)\n",
    "        else:\n",
    "            false_rows.append(row)\n",
    "    return true_rows, false_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['low', 'high', 'good', 'high', 'unstable', 'stable', 'mod-stable', 15, 'A'],\n",
       " ['high',\n",
       "  'high',\n",
       "  'excellent',\n",
       "  'high',\n",
       "  'unstable',\n",
       "  'stable',\n",
       "  'unstable',\n",
       "  15,\n",
       "  'A'],\n",
       " ['mid', 'high', 'good', 'mid', 'unstable', 'stable', 'unstable', 15, 'A']]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "#######\n",
    "# Demo:\n",
    "# Let's partition the training data based on whether rows are Red.\n",
    "true_rows, false_rows = partition(training_data, Question(7, 11))\n",
    "# This will contain all the 'Red' rows.\n",
    "true_rows[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['mid', 'mid', 'good', 'mid', 'unstable', 'stable', 'unstable', 10, 'S'],\n",
       " ['low', 'low', 'good', 'mid', 'unstable', 'stable', 'unstable', 10, 'S'],\n",
       " ['low', 'mid', 'good', 'mid', 'unstable', 'stable', 'stable', 10, 'A']]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# This will contain everything else.\n",
    "false_rows[:3]\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partitionnom(rows,column,data):\n",
    "    \"\"\"Partitions a dataset for nominal attributes.\n",
    "    \"\"\"\n",
    "    unique=list(unique_vals(data,column))\n",
    "    headers = [[] for i in range(len(unique))]\n",
    "    \n",
    "    for i in range(len(unique)):\n",
    "        for row in rows:\n",
    "            if row[column]==unique[i]:\n",
    "                headers[i].append(row)\n",
    "    return headers       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['high',\n",
       "  'high',\n",
       "  'excellent',\n",
       "  'high',\n",
       "  'unstable',\n",
       "  'stable',\n",
       "  'unstable',\n",
       "  10,\n",
       "  'A'],\n",
       " ['low', 'low', 'good', 'mid', 'unstable', 'stable', 'unstable', 10, 'A'],\n",
       " ['mid', 'mid', 'good', 'mid', 'stable', 'stable', 'unstable', 10, 'A']]"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "partitionnom(testing_data,6,data)[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(rows):\n",
    "    \"\"\"Calculate the Gini Impurity for a list of rows.\n",
    "\n",
    "    There are a few different ways to do this, I thought this one was\n",
    "    the most concise. See:\n",
    "    https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity\n",
    "    \"\"\"\n",
    "    counts = class_counts(rows)\n",
    "    impurity = 1\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        impurity -= prob_of_lbl**2\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self add\n",
    "def entropy(rows):\n",
    "    \"\"\"Calculate the Gain by Entropy for a list of rows.\n",
    "    \"\"\"\n",
    "    counts = class_counts(rows)\n",
    "    impurity = 0\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        impurity -= prob_of_lbl*log(prob_of_lbl,2)\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0\n0.0\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "# Demo:\n",
    "# Let's look at some example to understand how Gini Impurity works.\n",
    "#\n",
    "# First, we'll look at a dataset with no mixing.\n",
    "no_mixing = [['Apple'],\n",
    "              ['Apple']]\n",
    "class_counts(no_mixing)\n",
    "\n",
    "# this will return 0\n",
    "print(gini(no_mixing))\n",
    "\n",
    "#selfadd\n",
    "print(entropy(no_mixing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7999999999999998\n",
      "2.321928094887362\n"
     ]
    }
   ],
   "source": [
    "# Now, we'll look at a dataset with many different labels\n",
    "lots_of_mixing = [['Apple'],\n",
    "                  ['Orange'],\n",
    "                  ['Grape'],\n",
    "                  ['Grapefruit'],\n",
    "                  ['Blueberry']]\n",
    "# This will return 0.8\n",
    "print(gini(lots_of_mixing))\n",
    "#######\n",
    "\n",
    "#selfadd\n",
    "print(entropy(lots_of_mixing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_list(l):\n",
    "    count = 0\n",
    "    for e in l:\n",
    "        if isinstance(e, list):\n",
    "            count = count + 1 + count_list(e)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=count_list(partitionnom(training_data,0,data))-int(len(partitionnom(training_data,0,data)))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(left, right, current_uncertainty):\n",
    "    \"\"\"Information Gain for numerical attrubutes.\n",
    "\n",
    "    The uncertainty of the starting node, minus the weighted impurity of\n",
    "    two child nodes.\n",
    "    \"\"\"\n",
    "    p = float(len(left)) / (len(left) + len(right))\n",
    "    return current_uncertainty - p * gini(left) - (1 - p) * gini(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gainnom(header,current_uncertainty):\n",
    "    \"\"\"Information Gain for nominal attrubutes.\n",
    "\n",
    "    The uncertainty of the starting node, minus the weighted impurity of\n",
    "    two child nodes.\n",
    "    \"\"\"\n",
    "    gainnow=0\n",
    "    for lst in header:\n",
    "        p=float(len(lst))/(count_list(header)-len(header))\n",
    "        gainnow+=p*gini(lst)\n",
    "    return current_uncertainty-gainnow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selfadd\n",
    "def info_gainratio(left, right, current_uncertainty):\n",
    "    \"\"\"Information Gain Ratio for numerical attributes.\n",
    "\n",
    "    The uncertainty of the starting node, minus the weighted impurity of\n",
    "    two child nodes, and split by splitinfo.\n",
    "    \"\"\"\n",
    "    p = float(len(left)) / (len(left) + len(right))\n",
    "    gains = current_uncertainty - p * entropy(left)- (1 - p) *entropy(right)\n",
    "    return gains/(-p*log(p,2)-(1-p)*log((1-p),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selfadd\n",
    "def info_gainrationom(header,current_uncertainty):\n",
    "    \"\"\"Information Gain Ratio for nominal attrubutes.\n",
    "\n",
    "    The uncertainty of the starting node, minus the weighted impurity of\n",
    "    two child nodes.\n",
    "    \"\"\"\n",
    "    gainnow=0\n",
    "    split=0\n",
    "    \n",
    "    for lst in header:\n",
    "        p=float(len(lst))/(count_list(header)-len(header))\n",
    "        gainnow+=p*entropy(lst)\n",
    "        split-=p*log(p,2)\n",
    "    return (current_uncertainty-gainnow)/split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self add\n",
    "def auc(left, right,uni_label):\n",
    "    \"\"\"Calculate the auc for different partitions for numerical attributes.Doesn't need current uncertainty because doesn't depend\n",
    "    on parent node but only child nodes\n",
    "    \"\"\"\n",
    "    pos=uni_label[1]\n",
    "    neg=uni_label[0]\n",
    "\n",
    "    left=class_counts(left)\n",
    "    right=class_counts(right)\n",
    "    if pos not in left:\n",
    "        left[pos]=0\n",
    "    if neg not in left:\n",
    "        left[neg]=0\n",
    "    if pos not in right:\n",
    "        right[pos]=0\n",
    "    if neg not in right:\n",
    "        right[neg]=0\n",
    "\n",
    "    if left[pos]> right[pos] :\n",
    "        tp=left[pos]/(right[pos]+left[pos])\n",
    "        fp=left[neg]/(right[neg]+left[neg])\n",
    "    else:\n",
    "        tp=right[pos]/(right[pos]+left[pos])\n",
    "        fp=right[neg]/(right[neg]+left[neg])\n",
    "    return float(tp-fp+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self add\n",
    "def aucnom(header,uni_label):\n",
    "    \"\"\"Calculate the auc for different partitions for nominal attributes.Doesn't need current uncertainty because doesn't depend\n",
    "    on parent node but only child nodes\n",
    "    \"\"\"\n",
    "    pos=uni_label[1]\n",
    "    neg=uni_label[0]\n",
    "    posnum=0\n",
    "    negnum=0\n",
    "    auc=0\n",
    "    head=[]\n",
    "       \n",
    "    for ls in header: \n",
    "        v=class_counts(ls)\n",
    "        if pos not in v:v[pos]=0\n",
    "        if neg not in v:v[neg]=0\n",
    "        head.append(list(v.items()))\n",
    "\n",
    "    for i in range(len(header)):\n",
    "        v=class_counts(header[i])\n",
    "        if pos not in v:v[pos]=0\n",
    "        if neg not in v:v[neg]=0\n",
    "        val=v[pos]\n",
    "        posnum+=val\n",
    "        \n",
    "        for j in range(len(header)-1,i,-1):            \n",
    "            vcomp=class_counts(header[j])\n",
    "            if pos not in vcomp:vcomp[pos]=0\n",
    "            if neg not in vcomp:vcomp[neg]=0            \n",
    "            valcomp=vcomp[pos]\n",
    "            \n",
    "            if valcomp>val:\n",
    "                head[i]=list(vcomp.items())\n",
    "                head[j]=list(v.items())\n",
    "                \n",
    "    negnum=count_list(header)-len(header)-posnum\n",
    "\n",
    "    for ls in head:ls.sort()\n",
    "    if head[0][0][0]==pos: \n",
    "        num=0\n",
    "        notnum=1\n",
    "    else: \n",
    "        num=1\n",
    "        notnum=0\n",
    "        \n",
    "    for i in range(1,len(head)):\n",
    "        val=head[i][notnum][1]\n",
    "        val1=head[i][num][1]\n",
    "        valnext=0\n",
    "        for j in range(i):\n",
    "            add=head[j][num][1]\n",
    "            valnext+=add     \n",
    "        auc+=(val)*(2*valnext)+val1\n",
    "        \n",
    "    return float(auc)/(2*posnum*negnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39551020408163273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8435070855739036"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######\n",
    "# Demo:\n",
    "# Calculate the uncertainy of our training data.\n",
    "current_uncertainty = gini(training_data)\n",
    "print(current_uncertainty)\n",
    "\n",
    "#selfadd\n",
    "current_uncertainty2= entropy(training_data)\n",
    "current_uncertainty2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0024115153307699977\n",
      "0.0034457845905465855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27502579979360164"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(info_gainnom(partitionnom(training_data,0,data),current_uncertainty))\n",
    "print(info_gainrationom(partitionnom(training_data,0,data),current_uncertainty2))\n",
    "aucnom(partitionnom(training_data,0,data),uni_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006356764928193592\n",
      "0.014294237561698057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5598555211558307"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much information do we gain by partioning on 'Green'?\n",
    "true_rows, false_rows = partition(training_data, Question(7, 11))\n",
    "\n",
    "\n",
    "print(info_gain(true_rows, false_rows, current_uncertainty))\n",
    "#selfadd\n",
    "print(info_gainratio(true_rows, false_rows, current_uncertainty2))\n",
    "auc(true_rows,false_rows,uni_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002135462880804573\n",
      "0.06093145293200665\n",
      "0.49019607843137253\n"
     ]
    }
   ],
   "source": [
    "# What about if we partioned on 'Red' instead?\n",
    "true_rows, false_rows = partition(training_data, Question(7,7))\n",
    "\n",
    "print(info_gain(true_rows, false_rows, current_uncertainty))\n",
    "#selfadd\n",
    "print(info_gainratio(true_rows, false_rows, current_uncertainty2))\n",
    "print(auc(true_rows,false_rows,uni_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['low', 'high', 'good', 'high', 'unstable', 'stable', 'mod-stable', 15, 'A'],\n",
       " ['high',\n",
       "  'high',\n",
       "  'excellent',\n",
       "  'high',\n",
       "  'unstable',\n",
       "  'stable',\n",
       "  'unstable',\n",
       "  15,\n",
       "  'A']]"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "true_rows, false_rows = partition(training_data, Question(7,13))\n",
    "true_rows[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['mid', 'mid', 'good', 'mid', 'unstable', 'stable', 'unstable', 10, 'S'],\n",
       " ['low', 'low', 'good', 'mid', 'unstable', 'stable', 'unstable', 10, 'S']]"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "false_rows[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf:\n",
    "    \"\"\"A Leaf node classifies data.\n",
    "\n",
    "    This holds a dictionary of class (e.g., \"Apple\") -> number of times\n",
    "    it appears in the rows from the training data that reach this leaf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rows):\n",
    "        self.predictions=class_counts(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Node:\n",
    "    \"\"\"A Decision Node asks a question.\n",
    "\n",
    "    This holds a reference to the question, and to the two child nodes.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 true_branch,\n",
    "                 false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch=true_branch\n",
    "        self.false_branch = false_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Nodenom:\n",
    "    \"\"\"A Decision Node asks a question.\n",
    "\n",
    "    This holds a reference to the question, and to several child nodes.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 cutpoint,\n",
    "                 all_branch):\n",
    "        self.question = question\n",
    "        self.cutpoint= cutpoint\n",
    "        self.all_branch = all_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, spacing=\"\"):\n",
    "    \"\"\"World's most elegant tree printing function.\"\"\"\n",
    "\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        print (spacing + \"Predict\", node.predictions)\n",
    "        return\n",
    "\n",
    "    # Print the question at this node\n",
    "    elif isinstance(node,Decision_Node):\n",
    "        print (spacing + str(node.question))\n",
    "\n",
    "        # Call this function recursively on the true branch\n",
    "        print (spacing + '--> True:')\n",
    "        print_tree(node.true_branch, spacing + \"  \")\n",
    "\n",
    "        # Call this function recursively on the false branch\n",
    "        print (spacing + '--> False:')\n",
    "        print_tree(node.false_branch, spacing + \"  \")\n",
    "    \n",
    "    else:\n",
    "        print(spacing + str(node.question))\n",
    "        for i in range(len(node.all_branch)):\n",
    "            print(spacing+'-->'+str(node.cutpoint[i]))\n",
    "            print_tree(node.all_branch[i],spacing + \"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(rows,data):\n",
    "    \"\"\"Find the best question to ask by iterating over every feature / value\n",
    "    and calculating the information gain.\n",
    "    data usage : to find all possible levels of a column\n",
    "    \"\"\"\n",
    "    \n",
    "    best_gain = 0  # keep track of the best information gain\n",
    "    best_question = None  # keep train of the feature / value that produced it\n",
    "    num=None\n",
    "    column=None\n",
    "    cutpoint=None\n",
    "    branch=None\n",
    "    restart=None\n",
    "    \n",
    "    current_uncertainty = gini(rows)\n",
    "    n_features = len(rows[0]) - 1  # number of columns\n",
    "\n",
    "    #stopping criteria 1: all instances in node contain same class value\n",
    "    sameclass=set([row[-1] for row in rows])\n",
    "    if len(sameclass)==1:\n",
    "        num,branch,column,cutpoint,best_gain, best_question = num,branch,column,cutpoint,best_gain,best_question\n",
    "        return num,branch,column,cutpoint,best_gain, best_question\n",
    "    \n",
    "    for col in range(n_features):  # for each feature\n",
    "        values = set([row[col] for row in data])  # unique values in the column\n",
    "        values=list(values)\n",
    "        \n",
    "        # numerical variables!----------------------------------------------------\n",
    "        if is_numeric(values[0]):\n",
    "            for val in values:  # for each value deciding cutpoint\n",
    "\n",
    "                question = Question(col, val)\n",
    "\n",
    "                # try splitting the dataset\n",
    "                true_rows, false_rows = partition(rows, question)\n",
    "                \n",
    "                #stopping criteria 2: min number of instances in leaf\n",
    "                if len(true_rows) < 3 or len(false_rows) < 3:\n",
    "                    continue\n",
    "                    \n",
    "                #stopping criteria 3: all instances in node contain same attributes value\n",
    "                # Skip this split if it doesn't divide the dataset.                   \n",
    "                if len(true_rows) == 0 or len(false_rows) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Calculate the information gain from this split\n",
    "                gain = info_gain(true_rows, false_rows, current_uncertainty) \n",
    "                if gain > best_gain:\n",
    "                    num,branch,column,cutpoint,best_gain, best_question = True,2,col,val,gain, question\n",
    "\n",
    "        #nominal variables--------------------------------------------------------\n",
    "        else:\n",
    "            question=Questionnom(col)\n",
    "                \n",
    "            #stopping criteria 2: min number of instances in leaf\n",
    "            for ls in partitionnom(rows,col,data):\n",
    "                if len(ls) <3:\n",
    "                    restart=True\n",
    "                    break\n",
    "            if restart:\n",
    "                continue\n",
    "                \n",
    "            #stopping criteria 3: all instances in node contain same attributes value\n",
    "            #Skip this split if it doesn't divide the dataset.                \n",
    "            if len(partitionnom(rows,col,data)) == 1:\n",
    "                continue\n",
    "                    \n",
    "            # Calculate the information gain from this split\n",
    "            gain = info_gainnom(partitionnom(rows,col,data), current_uncertainty)\n",
    "            if gain > best_gain:\n",
    "                 num,branch,column,cutpoint,best_gain,best_question = False,len(partitionnom(rows,col,data)),col,values,gain,question\n",
    "        #--------------------------------------------------------------------------\n",
    "            # You actually can use '>' instead of '>=' here\n",
    "            # but I wanted the tree to look a certain way for our\n",
    "            # toy dataset.\n",
    "    #num:numerical or not, branch: number of branches, column: col. chose this time, cutpoint:all possible levels of that column\n",
    "    return num,branch,column,cutpoint,best_gain, best_question \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 2 7 15 0.006356764928193592 Is COMFORT >= 15?\n",
      "True 2 7 15 0.014294237561698057 Is COMFORT >= 15?\n",
      "True 2 7 15 0.5598555211558307 Is COMFORT >= 15?\n"
     ]
    }
   ],
   "source": [
    "# Demo:Find the best question to ask first for ourdataset.\n",
    "num,branch,col,cutpoint,best_gain, best_question = find_best_split(training_data,data)\n",
    "print(num,branch,col,cutpoint,best_gain,best_question)\n",
    "\n",
    "num,branch,col,cutpoint,best_gain, best_question = find_best_splitratio(training_data,data)\n",
    "print(num,branch,col,cutpoint,best_gain,best_question)\n",
    "\n",
    "num,branch,col,cutpoint,best_gain, best_question = find_best_splitauc(training_data,data)\n",
    "print(num,branch,col,cutpoint,best_gain,best_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_treegini(rows,data):\n",
    "    \"\"\"Builds the tree.\n",
    "\n",
    "    Rules of recursion: 1) Believe that it works. 2) Start by checking\n",
    "    for the base case (no further information gain). 3) Prepare for\n",
    "    giant stack traces.\n",
    "    \"\"\"\n",
    "    # Try partitioing the dataset on each of the unique attribute,\n",
    "    # calculate the information gain,\n",
    "    # and return the question that produces the highest gain.\n",
    "    num,branch,column,cutpoint,gain,question = find_best_split(rows,data)\n",
    "    # Base case: no further info gain\n",
    "    # Since we can ask no further questions,\n",
    "    # we'll return a leaf.\n",
    "    if gain == 0:\n",
    "        return Leaf(rows)\n",
    "\n",
    "    # If we reach here, we have found a useful feature / value\n",
    "    # to partition on.\n",
    "    if num==True:\n",
    "        true_rows, false_rows = partition(rows, question)\n",
    "        # Recursively build the true branch.\n",
    "        true_branch = build_treegini(true_rows,data)\n",
    "\n",
    "        # Recursively build the false branch.\n",
    "        false_branch = build_treegini(false_rows,data)\n",
    "        return Decision_Node(question,true_branch, false_branch)\n",
    "    \n",
    "    elif num==False:\n",
    "        store=[0]*branch\n",
    "        for i in range(branch):\n",
    "            store[i]=build_treegini(partitionnom(rows,column,data)[i],data)   \n",
    "        return Decision_Nodenom(question,cutpoint,store)\n",
    "        \n",
    "    # Return a Question node.\n",
    "    # This records the best feature / value to ask at this point,\n",
    "    # as well as the branches to follow\n",
    "    # depending on the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is COMFORT >= 15?\n",
      "--> True:\n",
      "  What's the L-SURF of instances?\n",
      "  -->low\n",
      "    Predict {'S': 2, 'A': 1}\n",
      "  -->mid\n",
      "    Predict {'A': 5, 'S': 3}\n",
      "  -->high\n",
      "    Predict {'A': 4, 'S': 1}\n",
      "--> False:\n",
      "  What's the L-CORE of instances?\n",
      "  -->low\n",
      "    Predict {'A': 8, 'S': 4}\n",
      "  -->mid\n",
      "    Predict {'S': 9, 'A': 27}\n",
      "  -->high\n",
      "    Predict {'A': 6}\n"
     ]
    }
   ],
   "source": [
    "my_treegini = build_treegini(training_data,data)\n",
    "print_tree(my_treegini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_splitratio(rows,data):\n",
    "    \"\"\"Find the best question to ask by iterating over every feature / value\n",
    "    and calculating the information gain.\"\"\"\n",
    "    \n",
    "    best_gain = 0  # keep track of the best information gain\n",
    "    best_question = None  # keep train of the feature / value that produced it\n",
    "    num=None\n",
    "    column=None\n",
    "    cutpoint=None\n",
    "    branch=None\n",
    "    restart=None\n",
    "    \n",
    "    current_uncertainty = entropy(rows)\n",
    "    n_features = len(rows[0]) - 1  # number of columns\n",
    "\n",
    "    #stopping criteria 1: all instances in node contain same class value\n",
    "    sameclass=set([row[-1] for row in rows])\n",
    "    \n",
    "    if len(sameclass)==1:\n",
    "        num,branch,column,cutpoint,best_gain, best_question = num,branch,column,cutpoint,best_gain,best_question\n",
    "        return num,branch,column,cutpoint,best_gain, best_question\n",
    "    \n",
    "    for col in range(n_features):  # for each feature\n",
    "                   \n",
    "        values = set([row[col] for row in data])  # unique values in the column\n",
    "        values=list(values)\n",
    "        \n",
    "        # numerical variables!----------------------------------------------------\n",
    "        if is_numeric(values[0]):\n",
    "            for val in values:  # for each value deciding cutpoint\n",
    "\n",
    "                question = Question(col, val)\n",
    "\n",
    "                # try splitting the dataset\n",
    "                true_rows, false_rows = partition(rows, question)\n",
    "                \n",
    "                #stopping criteria 2: number of instances in node is less than a threshold        \n",
    "                if len(true_rows) < 3 or len(false_rows) < 3:\n",
    "                    continue\n",
    "                \n",
    "                #stopping criteria 3: all instances in node contain same attributes value\n",
    "                # Skip this split if it doesn't divide the dataset.\n",
    "                if len(true_rows) == 0 or len(false_rows) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Calculate the information gainratio from this split\n",
    "                gain = info_gainratio(true_rows, false_rows, current_uncertainty) \n",
    "                if gain > best_gain:\n",
    "                    num,branch,column,cutpoint,best_gain, best_question = True,2,col,val,gain, question\n",
    "\n",
    "        #nominal variables--------------------------------------------------------\n",
    "        else:\n",
    "            question=Questionnom(col)\n",
    "                \n",
    "            #stopping criteria 2: number of instances in node is less than a threshold\n",
    "            for ls in partitionnom(rows,col,data):\n",
    "                if len(ls) <3:\n",
    "                    restart=True\n",
    "                    break\n",
    "            if restart:\n",
    "                continue\n",
    "                \n",
    "            #stopping criteria 3: all instances in node contain same attributes value\n",
    "            # Skip this split if it doesn't divide the dataset.                \n",
    "            if len(partitionnom(rows,col,data)) ==1:\n",
    "                    continue\n",
    "                    \n",
    "            # Calculate the information gainratio from this split\n",
    "            gain = info_gainrationom(partitionnom(rows,col,data), current_uncertainty)\n",
    "            if gain > best_gain:\n",
    "                 num,branch,column,cutpoint,best_gain,best_question = False,len(partitionnom(rows,col,data)),col,values,gain,question\n",
    "        #--------------------------------------------------------------------------\n",
    "            # You actually can use '>' instead of '>=' here\n",
    "            # but I wanted the tree to look a certain way for our\n",
    "            # toy dataset.\n",
    "\n",
    "    return num,branch,column,cutpoint,best_gain, best_question \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_treeratio(rows,data):\n",
    "    \"\"\"Builds the tree.\n",
    "\n",
    "    Rules of recursion: 1) Believe that it works. 2) Start by checking\n",
    "    for the base case (no further information gain). 3) Prepare for\n",
    "    giant stack traces.\n",
    "    \"\"\"\n",
    "    # Try partitioing the dataset on each of the unique attribute,\n",
    "    # calculate the information gain,\n",
    "    # and return the question that produces the highest gain.\n",
    "    num,branch,column,cutpoint,gain,question = find_best_splitratio(rows,data)\n",
    "    # Base case: no further info gain\n",
    "    # Since we can ask no further questions,\n",
    "    # we'll return a leaf.\n",
    "    if gain == 0:\n",
    "        return Leaf(rows)\n",
    "\n",
    "    # If we reach here, we have found a useful feature / value\n",
    "    # to partition on.\n",
    "    elif num==True:\n",
    "        true_rows, false_rows = partition(rows, question)\n",
    "\n",
    "        # Recursively build the true branch.\n",
    "        true_branch = build_treeratio(true_rows,data)\n",
    "\n",
    "        # Recursively build the false branch.\n",
    "        false_branch = build_treeratio(false_rows,data)\n",
    "        return Decision_Node(question,true_branch, false_branch)\n",
    "    \n",
    "    elif num==False:\n",
    "        store=[0]*branch\n",
    "        for i in range(branch):\n",
    "            store[i]=build_treeratio(partitionnom(rows,column,data)[i],data)   \n",
    "        return Decision_Nodenom(question,cutpoint,store)\n",
    "        \n",
    "    # Return a Question node.\n",
    "    # This records the best feature / value to ask at this point,\n",
    "    # as well as the branches to follow\n",
    "    # depending on the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is COMFORT >= 15?\n",
      "--> True:\n",
      "  What's the L-SURF of instances?\n",
      "  -->low\n",
      "    Predict {'S': 2, 'A': 1}\n",
      "  -->mid\n",
      "    Predict {'A': 5, 'S': 3}\n",
      "  -->high\n",
      "    Predict {'A': 4, 'S': 1}\n",
      "--> False:\n",
      "  What's the L-CORE of instances?\n",
      "  -->low\n",
      "    Predict {'A': 8, 'S': 4}\n",
      "  -->mid\n",
      "    Predict {'S': 9, 'A': 27}\n",
      "  -->high\n",
      "    Predict {'A': 6}\n"
     ]
    }
   ],
   "source": [
    "my_treeratio = build_treeratio(training_data,data)\n",
    "print_tree(my_treeratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_splitauc(rows,data):\n",
    "    \"\"\"Find the best question to ask by iterating over every feature / value\n",
    "    and calculating the information gain.\"\"\"\n",
    "    \n",
    "    best_gain = 0  # keep track of the best information gain\n",
    "    best_question = None  # keep train of the feature / value that produced it\n",
    "    num=None\n",
    "    column=None\n",
    "    cutpoint=None\n",
    "    branch=None\n",
    "    restart=None\n",
    "\n",
    "    n_features = len(rows[0]) - 1  # number of columns\n",
    "\n",
    "    #stopping criteria 1: all instances in node contain same class value\n",
    "    sameclass=set([row[-1] for row in rows])\n",
    "    if len(sameclass)==1:\n",
    "        num,branch,column,cutpoint,best_gain, best_question = num,branch,column,cutpoint,best_gain,best_question\n",
    "        return num,branch,column,cutpoint,best_gain, best_question\n",
    "    \n",
    "    \n",
    "    for col in range(n_features):  # for each feature\n",
    "                   \n",
    "        values = set([row[col] for row in data])  # unique values in the column\n",
    "        values=list(values)\n",
    "        \n",
    "        # numerical variables!----------------------------------------------------\n",
    "        if is_numeric(values[0]):\n",
    "            for val in values:  # for each value deciding cutpoint\n",
    "\n",
    "                question = Question(col, val)\n",
    "\n",
    "                # try splitting the dataset\n",
    "                true_rows, false_rows = partition(rows, question)\n",
    "                \n",
    "                #stopping criteria 2: number of instances in node is less than a threshold        \n",
    "                if len(true_rows) < 3 or len(false_rows) < 3:\n",
    "                    continue\n",
    "                \n",
    "                #stopping criteria 3: all instances in node contain same attributes value\n",
    "                # Skip this split if it doesn't divide the dataset.\n",
    "                if len(true_rows) == 0 or len(false_rows) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Calculate the information gain from this split\n",
    "                gain = auc(true_rows, false_rows,uni_label) \n",
    "                if gain > best_gain:\n",
    "                    num,branch,column,cutpoint,best_gain, best_question = True,2,col,val,gain, question\n",
    "\n",
    "        #nominal variables--------------------------------------------------------\n",
    "        else:\n",
    "            question=Questionnom(col)\n",
    "                \n",
    "            #stopping criteria 2: number of instances in node is less than a threshold\n",
    "            for ls in partitionnom(rows,col,data):\n",
    "                if len(ls) <3:\n",
    "                    restart=True\n",
    "                    break\n",
    "            if restart:\n",
    "                continue\n",
    "                \n",
    "            #stopping criteria 3: all instances in node contain same attributes value\n",
    "            # Skip this split if it doesn't divide the dataset.                \n",
    "            if len(partitionnom(rows,col,data)) ==1:\n",
    "                    continue\n",
    "                    \n",
    "            # Calculate the information gain from this split\n",
    "            gain = aucnom(partitionnom(rows,col,data),uni_label)\n",
    "            if gain > best_gain:\n",
    "                 num,branch,column,cutpoint,best_gain,best_question = False,len(partitionnom(rows,col,data)),col,values,gain,question\n",
    "        #--------------------------------------------------------------------------\n",
    "            # You actually can use '>' instead of '>=' here\n",
    "            # but I wanted the tree to look a certain way for our\n",
    "            # toy dataset.\n",
    "\n",
    "    return num,branch,column,cutpoint,best_gain, best_question \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_treeauc(rows,data):\n",
    "    num,branch,column,cutpoint,gain,question = find_best_splitauc(rows,data)\n",
    "    # Base case: no further gain(3 stopping criterions met,or gain is truly 0)\n",
    "    if gain == 0:\n",
    "        return Leaf(rows)\n",
    "\n",
    "    # If we reach here, we have found a useful feature / value\n",
    "    # to partition on.\n",
    "    if num==True:\n",
    "        true_rows, false_rows = partition(rows, question)\n",
    "\n",
    "        # Recursively build the true branch.\n",
    "        true_branch = build_treeauc(true_rows,data)\n",
    "\n",
    "        # Recursively build the false branch.\n",
    "        false_branch = build_treeauc(false_rows,data)\n",
    "        \n",
    "        return Decision_Node(question,true_branch, false_branch)\n",
    "    \n",
    "    elif num==False:\n",
    "        store=[0]*branch\n",
    "        for i in range(branch):\n",
    "            store[i]=build_treeauc(partitionnom(rows,column,data)[i],data)   \n",
    "        return Decision_Nodenom(question,cutpoint,store)\n",
    "        \n",
    "    # Return a Question node.\n",
    "    # This records the best feature / value to ask at this point,\n",
    "    # as well as the branches to follow\n",
    "    # depending on the answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is COMFORT >= 15?\n",
      "--> True:\n",
      "  What's the L-CORE of instances?\n",
      "  -->low\n",
      "    Predict {'A': 2, 'S': 1}\n",
      "  -->mid\n",
      "    Predict {'A': 7, 'S': 3}\n",
      "  -->high\n",
      "    Predict {'A': 1, 'S': 2}\n",
      "--> False:\n",
      "  What's the L-O2 of instances?\n",
      "  -->excellent\n",
      "    What's the L-SURF of instances?\n",
      "    -->low\n",
      "      Predict {'A': 5, 'S': 1}\n",
      "    -->mid\n",
      "      Predict {'A': 13, 'S': 3}\n",
      "    -->high\n",
      "      Predict {'A': 3, 'S': 1}\n",
      "  -->good\n",
      "    Predict {'A': 20, 'S': 8}\n"
     ]
    }
   ],
   "source": [
    "my_treeauc = build_treeauc(training_data,data)\n",
    "print_tree(my_treeauc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(row, node):\n",
    "    \"\"\"See the 'rules of recursion' above.\"\"\"\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        return node.predictions\n",
    "\n",
    "    # Decide whether to follow the true-branch or the false-branch.\n",
    "    # Compare the feature / value stored in the node,\n",
    "    # to the example we're considering.\n",
    "    if isinstance(node,Decision_Node):\n",
    "        if node.question.match(row):\n",
    "            return classify(row, node.true_branch)\n",
    "        else:        \n",
    "            return classify(row, node.false_branch)\n",
    "        \n",
    "    else:\n",
    "        for i in range(len(node.all_branch)):\n",
    "            if node.cutpoint[i] == row[node.question.column]:\n",
    "                return classify(row,node.all_branch[i])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 20, 'S': 8}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######\n",
    "# Demo:\n",
    "# The tree predicts the 1st row of our\n",
    "# training data is an apple with confidence 1.\n",
    "\n",
    "classify(training_data[5], my_treeauc)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_leaf(counts,training_data):\n",
    "    \"\"\"A nicer way to print the predictions at a leaf.\"\"\"\n",
    "    total = sum(counts.values()) * 1.0\n",
    "    probs = {}\n",
    "    finalvalue=0\n",
    "    for lbl in counts.keys():\n",
    "        probs[lbl] = float(counts[lbl] / total )\n",
    "    probs=list(probs.items())\n",
    "    probs.sort()\n",
    "    \n",
    "    denomval=[0]*2\n",
    "    \n",
    "    for ls in training_data:\n",
    "        if ls[-1]=='S':\n",
    "            denomval[1]+=1\n",
    "        else:\n",
    "            denomval[0]+=1\n",
    "    \n",
    "    if len(probs)==1:\n",
    "        if probs[0][0]=='A':\n",
    "            return probs[0][0]\n",
    "        else:\n",
    "            return probs[1][0]\n",
    "    \n",
    "    #neg/(pos+neg)*P(+|x)>pos/(pos+neg)*P(-|x)\n",
    "    if denomval[0]/len(training_data)*float(probs[1][1])>denomval[1]/len(training_data)*float(probs[0][1]):\n",
    "        return probs[1][0]\n",
    "    else:\n",
    "        return probs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######\n",
    "# Demo:\n",
    "# Printing that a bit nicer\n",
    "print_leaf(classify(training_data[54], my_treeauc),training_data)\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######\n",
    "# Demo:\n",
    "# On the second example, the confidence is lower\n",
    "print_leaf(classify(training_data[5], my_treeauc),training_data)\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: A. Predicted: S\n",
      "Actual: A. Predicted: A\n",
      "Actual: A. Predicted: A\n",
      "Actual: S. Predicted: A\n",
      "Actual: S. Predicted: A\n",
      "Actual: S. Predicted: A\n",
      "Actual: A. Predicted: A\n",
      "Actual: A. Predicted: A\n",
      "Actual: S. Predicted: S\n",
      "Actual: A. Predicted: S\n",
      "Actual: A. Predicted: S\n",
      "Actual: A. Predicted: A\n",
      "Actual: A. Predicted: S\n",
      "Actual: A. Predicted: A\n",
      "Actual: A. Predicted: A\n",
      "Actual: A. Predicted: A\n",
      "Actual: S. Predicted: A\n",
      "Actual: A. Predicted: S\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "acc=0\n",
    "for row in testing_data:\n",
    "    print (\"Actual: %s. Predicted: %s\" %\n",
    "           (row[-1], print_leaf(classify(row, my_treegini),training_data)))\n",
    "    if row[-1] == print_leaf(classify(row, my_treegini),training_data):\n",
    "        acc+=1\n",
    "print(float(acc)/len(testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: A. Predicted: S\n",
      "Actual: A. Predicted: A\n",
      "Actual: A. Predicted: A\n",
      "Actual: S. Predicted: A\n",
      "Actual: S. Predicted: A\n",
      "Actual: S. Predicted: A\n",
      "Actual: A. Predicted: A\n",
      "Actual: A. Predicted: A\n",
      "Actual: S. Predicted: S\n",
      "Actual: A. Predicted: S\n",
      "Actual: A. Predicted: S\n",
      "Actual: A. Predicted: A\n",
      "Actual: A. Predicted: S\n",
      "Actual: A. Predicted: A\n",
      "Actual: A. Predicted: A\n",
      "Actual: A. Predicted: A\n",
      "Actual: S. Predicted: A\n",
      "Actual: A. Predicted: S\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "acc=0\n",
    "for row in testing_data:\n",
    "    print (\"Actual: %s. Predicted: %s\" %\n",
    "           (row[-1], print_leaf(classify(row, my_treeratio),training_data)))\n",
    "    if row[-1] == print_leaf(classify(row, my_treeratio),training_data):\n",
    "        acc+=1\n",
    "print(float(acc)/len(testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: A. Predicted: S\n",
      "Actual: A. Predicted: A\n",
      "Actual: A. Predicted: A\n",
      "Actual: S. Predicted: A\n",
      "Actual: S. Predicted: A\n",
      "Actual: S. Predicted: A\n",
      "Actual: A. Predicted: S\n",
      "Actual: A. Predicted: A\n",
      "Actual: S. Predicted: A\n",
      "Actual: A. Predicted: S\n",
      "Actual: A. Predicted: A\n",
      "Actual: A. Predicted: A\n",
      "Actual: A. Predicted: S\n",
      "Actual: A. Predicted: S\n",
      "Actual: A. Predicted: A\n",
      "Actual: A. Predicted: A\n",
      "Actual: S. Predicted: A\n",
      "Actual: A. Predicted: S\n",
      "0.3888888888888889\n"
     ]
    }
   ],
   "source": [
    "acc=0\n",
    "for row in testing_data:\n",
    "    print (\"Actual: %s. Predicted: %s\" %\n",
    "           (row[-1], print_leaf(classify(row, my_treeauc),training_data)))\n",
    "    if row[-1] == print_leaf(classify(row, my_treeauc),training_data):\n",
    "        acc+=1\n",
    "print(float(acc)/len(testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome(testing_data,my_tree,training_data):\n",
    "    pos=0\n",
    "    neg=0\n",
    "    acc=0\n",
    "    tp=0\n",
    "    fp=0\n",
    "    pre=0\n",
    "    f1=0\n",
    "    accuracy=None\n",
    "    tprate=None\n",
    "    fprate=None\n",
    "    precision=None\n",
    "    f1=None\n",
    "    \n",
    "    for row in testing_data:       \n",
    "        if row[-1] =='S':\n",
    "            pos+=1\n",
    "        else:\n",
    "            neg+=1\n",
    "        if row[-1] == print_leaf(classify(row, my_tree),training_data):\n",
    "            acc+=1\n",
    "            if row[-1]=='S': \n",
    "                tp+=1\n",
    "        else:\n",
    "            if row[-1]=='A': \n",
    "                fp+=1\n",
    "                \n",
    "        accuracy=acc/len(testing_data)\n",
    "        if pos!=0:\n",
    "            tprate=tp/pos\n",
    "        if neg!=0:\n",
    "            fprate=fp/neg\n",
    "        if tp!=0 or fp!=0:\n",
    "            precision=tp/(tp+fp)\n",
    "        if tprate!=None and precision!=None and tprate!=0 and precision!=0:\n",
    "            f1=1/((1/tprate)+(1/precision))/2\n",
    "            \n",
    "    return print(\"Accuracy: %s. TP rate: %s. FP rate: %s. Precision: %s. F1: %s\" % (accuracy,tprate,fprate,precision,f1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5909090909090909. TP rate: 0.0. FP rate: 0.13333333333333333. Precision: 0.0. F1: None\n",
      "Accuracy: 0.5909090909090909. TP rate: 0.0. FP rate: 0.13333333333333333. Precision: 0.0. F1: None\n",
      "Accuracy: 0.5909090909090909. TP rate: 0.0. FP rate: 0.13333333333333333. Precision: 0.0. F1: None\n",
      "Accuracy: 0.45454545454545453. TP rate: 0.8. FP rate: 0.6470588235294118. Precision: 0.26666666666666666. F1: 0.1\n",
      "Accuracy: 0.45454545454545453. TP rate: 0.8. FP rate: 0.6470588235294118. Precision: 0.26666666666666666. F1: 0.1\n",
      "Accuracy: 0.36363636363636365. TP rate: 0.6. FP rate: 0.7058823529411765. Precision: 0.2. F1: 0.075\n",
      "Accuracy: 0.36363636363636365. TP rate: 0.5. FP rate: 0.7142857142857143. Precision: 0.2857142857142857. F1: 0.09090909090909091\n",
      "Accuracy: 0.36363636363636365. TP rate: 0.5. FP rate: 0.7142857142857143. Precision: 0.2857142857142857. F1: 0.09090909090909091\n",
      "Accuracy: 0.3181818181818182. TP rate: 0.25. FP rate: 0.6428571428571429. Precision: 0.18181818181818182. F1: 0.05263157894736842\n",
      "Accuracy: 0.7727272727272727. TP rate: 0.5. FP rate: 0.16666666666666666. Precision: 0.4. F1: 0.1111111111111111\n",
      "Accuracy: 0.6818181818181818. TP rate: 0.5. FP rate: 0.2777777777777778. Precision: 0.2857142857142857. F1: 0.09090909090909091\n",
      "Accuracy: 0.6818181818181818. TP rate: 0.25. FP rate: 0.2222222222222222. Precision: 0.2. F1: 0.05555555555555555\n"
     ]
    }
   ],
   "source": [
    "#ten fold cross validation\n",
    "random.seed(100)\n",
    "randtran=random.sample(range(88),88)\n",
    "\n",
    "for i in range(4):   \n",
    "    testing_data=[data[k] for k in randtran[i*22:i*22+22]]\n",
    "    training_data=[data[k] for k in range(88) if k not in randtran[i*22:i*22+22]]\n",
    "\n",
    "    my_treegini=build_treegini(training_data,data)\n",
    "    my_treeratio = build_treeratio(training_data,data)\n",
    "    my_treeauc = build_treeauc(training_data,data) \n",
    "    outcome(testing_data,my_treegini,training_data)\n",
    "    outcome(testing_data,my_treeratio,training_data)\n",
    "    outcome(testing_data,my_treeauc,training_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python394jvsc74a57bd0fe06f96167c4fc64a78c238d993189072a4e72b444216e36203d6f96126eaf0a",
   "display_name": "Python 3.9.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}